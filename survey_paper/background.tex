% !TEX root = ./survey_paper.tex

\section{The Brick Wall}
\label{sec:background}

As mentioned in Section~\ref{sec:intro}, Manferdelli \textit{et al.}~\cite{4484943} reports on the ``Brick Wall'', which they split into three ``sub-walls'' in their report.
While these are not the only difficulties in the never-ending quest for increasing computing power, these three are the big, well-known ones that keep computing power from continuing the prediction of Moore's law.

This section lists the three and explain how they prevent the continuation of Moore's law, which predicts the exponential rise at constant cost every 18 months.


\subsection{Power/Heat Wall}

The miniaturization of components used to build CPUs has been one of the biggest driving forces, if not the biggest one, behind Moore's Law.

As miniaturization decreased the size of the components used in a CPU's die area, so did the cost per component in the CPU.
As the size decreased, the number of units possible to cram into the integrated circuit (IC) increased.
With the number of transistors in the CPU increased, so was the increase in clock cycles (frequency) permitted, directly affecting the number of instructions possible to execute per second.
The heat generated when powering the miniaturized components, at higher frequencies, increased exponentially due to power leakage of transistors at these small sizes, and the density of components generating heat in the IC when in use.

Hence, the power demands required to keep the units cooled down to acceptable working temperatures far exceed the power demands of utilization.
This gave rise to the term ``Dark Silicon'', coined by the strategy of turning off select subsections of the CPU's die area during utilization, so as to keep the temperature sufficiently low to function.

So far, notwithstanding major breakthroughs in processor die materials or electronics avoiding the ``Heat Wall'' generated by the ``Power Wall'', the main tactic to counter this problem has been to spread the workload across multiple cores.
Often utilizing specialized heterogeneous cores such as GPUs for suited applications.

\subsection{Instruction-Level-Parallelism Wall}

Instruction-Level-Parallelism (ILP) is the simultaneous execution of multiple instructions in a sequential program, so long as there are no data dependencies between the instructions.

One approach to accomplish ILP has been Very Large Instruction Words (VLIW), requiring the compiler to see the opportunities for ILP in a program during the offline compilation.
VLIWs reliance on the compiler to perform optimally has hindered the approach's widespread use and acceptance on HPC platform and personal use platform, which often run very diverse programs.
However, on embedded platforms, typically those that only execute a few different programs, though they may need to execute them frequently and/or efficiently, has been where the VLIW shines, as there is little variety in the programs run on these devices.

While this is now changing with embedded processors in devices such as smartphones being more and more utilized for a variety of tasks, there is one other approach to ILP that has been much used on the personal and HPC platforms.

Out-of-Order execution has been a hardware optimization that permits the CPUs themselves to see and act on potential ILP in a program at run-time, namely online, as opposed to VLIW, which works offline.

O-o-O cores read their given instructions as they come in, and if they find instructions that have no dependencies among the ones loaded in cache, they use whatever free resources\footnote{Such as those available when there's been a cache miss.} to execute CPU instructions that the program has not yet reached.

However, while ILP works, to the degree limited by cache size and hardware, it's big limitation lies in the majority of programs being written as serial programs.
Thus hindering the further speedup through utilization of O-o-O execution.

The main tactic used to counter the ``ILP Wall'', has been to write more parallel programs.
Programs whose algorithms rely less on sequential steps, and more on discrete, independent steps, ``sewn together'' at critical points in the program than serial programs.

\subsection{Memory Wall}

Finally, the ``Memory Wall'' has come as a consequence of the increasing speed and throughput of the processor diverging with the accompanying required speed and throughput from memory, which has not had the same exponential growth.
Wulf and McKee~\cite{Wulf:1995:HMW:216585.216588} detail the then impending ``Memory Wall'', and explain in their paper how, even given the most optimistic assumptions, it will come to pass with the increasing difference in throughput of execution of instructions on the CPU, and the throughput bandwidth in memory.

The memory hierarchy is an attempt to alleviate, and to a certain degree \textit{hide} the difference in throughput bandwidth between larger, slower memory technologies (like HDDs), and faster, smaller memory technologies like caches.

However, the effect of the memory hierarchy is dependent on the \textit{correct} memory being loaded from the lower, high-capacity ends of the hierarchy to faster, low-capacity upper ones, so as to be accessible as soon as possible, if not even before execution (when the CPU needs the data located in that particular part of memory).
Operating Systems (OSs) have long worked on hiding this, and in today's technology, do a good job of hiding the discrepancies from their human users on personal computers and devices.

Nevertheless, on HPC systems, which are not used for the typical and quick programs most frequently utilized on personal computing systems, the programs being run can take up to days, if not weeks or months to execute successfully (and continuously) from start to finish.

Thus, when high thermal, geological, meteorological calculations, simulations need to process amounts of data on the scale of terabytes, $8*10^{12}$ bits, all of that data has to pass through the fastest part of the memory hierarchy, the registers in a CPU.
The registers are also the ones with the lowest memory capacity, often on the order of $X$ registers, each with $64$ bits or less, with $X < 64$ in today's most modern and common high-end CPUs.
Hence, the advent of caches, and prefetching, in an attempt to have the data required in the next execution cycle of the CPU as close to the registers in the hierarchy as possible.

However, while the memory hierarchy is big and includes several very different memory technologies, the ``Memory Wall'' most often refers to the discrepancy between on-chip memory bandwidth, and off-chip memory bandwidth.
In simpler terms, when the data is transferred between ICs.

Besides the aforementioned caches and prefetching, there are several main tactics to combat the ``Memory Wall'', such as the exploitation of temporal and spatial locality in caches. One other tactic is writing the programs to be more parallel, permitting the load of the memory bandwidth on the CPU to be spread across multiple CPUs, if not also CPU cores.
